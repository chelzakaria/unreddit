{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import LlamaCPP, KeyBERTInspired\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from cuml.cluster import HDBSCAN\n",
    "from cuml.manifold import UMAP\n",
    "from bertopic import BERTopic\n",
    "from llama_cpp import Llama\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom stop words list\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words.extend(set(stopwords.words('french')))\n",
    "stop_words.extend(set(stopwords.words('arabic'))) \n",
    "# add custom darija stop words\n",
    "stop_words.extend(['هادشي','علاش','Machi', 'Gha', 'Dyal','chi','li','mn','3la','ana','wach','wla','bghit','bach','ila','rah','m3a','nta','ghir','dial','الله','راه','شي','ديال','هاد','او','ماشي','باش','انا','اللي','حاجة','ليا','عندي','ghadi','b7al','3liha','wakha','ba9i','3lih','3lik','3lach','liha','mazal','ليك','ال','الل','بلا','machi','dyal','kan','ra','howa','hadchi','lik','gha','walakin','daba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          None\n",
       "1          None\n",
       "2          None\n",
       "3          None\n",
       "4          None\n",
       "           ... \n",
       "1203987    None\n",
       "1203988    None\n",
       "1203989    None\n",
       "1203990    None\n",
       "1203991    None\n",
       "Name: body, Length: 1203992, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load comments\n",
    "with open('../data/cleaned/comments.csv', 'r', encoding='utf-8') as file:\n",
    "    comments_df = pd.read_csv(file, low_memory=False)\n",
    "comments_df.fillna('', inplace=True)\n",
    "comments_df.sort_values(by='created_utc', inplace=True, ignore_index=True)\n",
    "comments = []\n",
    "comments_df.body.apply(lambda x: comments.append(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "88562    None\n",
       "88563    None\n",
       "88564    None\n",
       "88565    None\n",
       "88566    None\n",
       "Name: body, Length: 88567, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load submissions\n",
    "with open('../data/cleaned/submissions.csv', 'r', encoding='utf-8') as file:\n",
    "    submissions = pd.read_csv(file)    \n",
    "submissions.fillna('', inplace=True)\n",
    "# drop rows where AutoModerator is the author\n",
    "submissions = submissions[submissions['author'] != 'AutoModerator']\n",
    "# concat title and selftext\n",
    "submissions['body'] = submissions['title'] + ' ' + submissions['selftext']\n",
    "submissions.sort_values(by='created_utc', inplace=True, ignore_index=True)\n",
    "posts = []\n",
    "submissions.body.apply(lambda x: posts.append(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer_model\n",
    "vectorizer_model = CountVectorizer(stop_words=stop_words, min_df=10, ngram_range=(1, 2))\n",
    "\n",
    "# define umap_model and hdbscan_model for GPU acceleration\n",
    "umap_model = UMAP(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True, prediction_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-calculated embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(posts + comments, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Llama model for LLM representation\n",
    "# wget https://huggingface.co/TheBloke/zephyr-7B-alpha-GGUF/resolve/main/zephyr-7b-alpha.Q4_K_M.gguf -P ../models \n",
    "llm = Llama(model_path=\"../models/zephyr-7b-alpha.Q4_K_M.gguf\", n_gpu_layers=-1, n_ctx=4096, stop=[\"Q:\", \"\\n\"],verbose=False)\n",
    "\n",
    "representation_model = {\n",
    "   \"KeyBERT\": KeyBERTInspired(),\n",
    "   \"LLM\": LlamaCPP(llm),\n",
    "}\n",
    "\n",
    "topic_model = BERTopic(min_topic_size=50, embedding_model=embedding_model, representation_model=representation_model, verbose=True, language=\"multilingual\", vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.fit_transform(posts + comments, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.save(\"../models/bertopic-llama_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chel/folder/RedDash/env/lib/python3.11/site-packages/cupy/_creation/from_data.py:75: PerformanceWarning:\n",
      "\n",
      "Using synchronous transfer as pinned memory (1985370624 bytes) could not be allocated. This generally occurs because of insufficient host memory. The original error was: cudaErrorMemoryAllocation: out of memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic.load(\"../models/bertopic-llama_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map topic result to data and save it to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.drop(columns=['author', 'author_flair_text', 'title', 'selftext', 'link_flair_text', 'over_18', 'num_comments', 'hide_score'], inplace=True) \n",
    "comments_df.drop(columns=['author', 'author_flair_text', 'link_id', 'parent_id','score_hidden'], inplace=True)\n",
    "\n",
    "# merge submissions and comments\n",
    "submissions['type'] = 'submission'\n",
    "comments_df['type'] = 'comment'\n",
    "data = pd.concat([submissions, comments_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = topic_model.get_document_info(posts + comments)\n",
    "topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, topic_docs], axis=1)\n",
    "data.drop(columns=['Document','Name','Representation','KeyBERT','LLM','Representative_Docs','Top_n_words','Representative_document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/results/data.csv', index=False)\n",
    "# topic_info.to_csv('../data/results/topic_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some cleaning on the LLM column\n",
    "topic_info['LLM'] = topic_info['LLM'].str[0]\n",
    "topic_info['LLM'] = topic_info['LLM'].apply(lambda x: re.sub(r'Explanation:.*', '', x, flags=re.DOTALL).strip())\n",
    "topic_info['LLM'] = topic_info['LLM'].apply(lambda x: re.sub(r'\\(note.*', '', x, flags=re.DOTALL).strip())\n",
    "topic_info['LLM'] = topic_info['LLM'].apply(lambda x: re.sub(r'\"', '', x))\n",
    "topic_info['LLM'] = topic_info['LLM'].apply(lambda x: re.sub(r' \\'', '', x))\n",
    "topic_info['LLM'] = topic_info['LLM'].apply(lambda x: re.sub(r'\\' ', '', x))\n",
    "#....\n",
    "# more cleaning if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info.to_csv('../data/results/topic_info_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
